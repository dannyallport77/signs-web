name: Daily Database Backup to R2

on:
  schedule:
    # Run every day at 3:00 AM UTC
    - cron: '0 3 * * *'
  # Allow manual trigger
  workflow_dispatch:

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Install PostgreSQL 17 client
        run: |
          sudo apt-get update
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          sudo apt-get update
          sudo apt-get install -y postgresql-client-17

      - name: Create database backup
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          FILENAME="signs-backup-${TIMESTAMP}.dump"
          
          echo "Creating backup: ${FILENAME}"
          pg_dump "${DATABASE_URL}" -F c -f "${FILENAME}"
          
          # Verify backup was created and has content
          if [ ! -s "${FILENAME}" ]; then
            echo "Error: Backup file is empty or was not created"
            exit 1
          fi
          
          FILESIZE=$(stat -c%s "${FILENAME}")
          echo "Backup created successfully: ${FILENAME} (${FILESIZE} bytes)"
          
          # Export for next step
          echo "BACKUP_FILENAME=${FILENAME}" >> $GITHUB_ENV
          echo "BACKUP_FILESIZE=${FILESIZE}" >> $GITHUB_ENV

      - name: Upload to Cloudflare R2
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          # Install AWS CLI (R2 is S3-compatible)
          pip install awscli
          
          # Configure AWS CLI for R2
          aws configure set aws_access_key_id "${R2_ACCESS_KEY_ID}"
          aws configure set aws_secret_access_key "${R2_SECRET_ACCESS_KEY}"
          aws configure set region auto
          
          # Upload to R2
          echo "Uploading ${BACKUP_FILENAME} to R2..."
          aws s3 cp "${BACKUP_FILENAME}" "s3://${R2_BUCKET}/database-backups/${BACKUP_FILENAME}" \
            --endpoint-url "${R2_ENDPOINT}"
          
          echo "✅ Backup uploaded successfully to R2"

      - name: Cleanup old backups (keep last 30)
        env:
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
        run: |
          # List all backups, sort by date, keep only last 30
          aws s3 ls "s3://${R2_BUCKET}/database-backups/" \
            --endpoint-url "${R2_ENDPOINT}" | \
            sort -r | \
            tail -n +31 | \
            awk '{print $4}' | \
            while read file; do
              if [ -n "$file" ]; then
                echo "Deleting old backup: $file"
                aws s3 rm "s3://${R2_BUCKET}/database-backups/$file" \
                  --endpoint-url "${R2_ENDPOINT}"
              fi
            done
          
          echo "✅ Cleanup complete"

      - name: Summary
        run: |
          echo "## Database Backup Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Filename:** ${BACKUP_FILENAME}" >> $GITHUB_STEP_SUMMARY
          echo "- **Size:** ${BACKUP_FILESIZE} bytes" >> $GITHUB_STEP_SUMMARY
          echo "- **Uploaded to:** R2 bucket \`${R2_BUCKET}\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp:** $(date -u)" >> $GITHUB_STEP_SUMMARY
        env:
          R2_BUCKET: ${{ secrets.R2_BUCKET }}
